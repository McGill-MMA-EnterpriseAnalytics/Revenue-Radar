{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Processing Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This document details the procedures used for preparing and processing the data in the machine learning project. The data is systematically divided, transformed, and cleaned to ensure optimal input for modeling.\n",
    "\n",
    "## Data Splitting\n",
    "\n",
    "Data is split into two distinct sets based on a cutoff date:\n",
    "- **Training Data**: All records before May 1, 2017, are used for model training.\n",
    "- **Testing Data**: Records from May 1, 2017, onwards are reserved for testing the model.\n",
    "\n",
    "## Data Preparation Steps\n",
    "\n",
    "### One-hot Encoding\n",
    "Categorical data from the 'source' column is transformed into a numerical format using one-hot encoding. \n",
    "\n",
    "### Dimensionality Reduction with PCA\n",
    "Principal Component Analysis (PCA) is applied to the one-hot encoded data to reduce its dimensionality. This process helps in maintaining as much variance in the 'source' feature while reducing the dimensionality that results from one-hot encoding.\n",
    "\n",
    "### Data Cleaning and Feature Engineering\n",
    "Several steps are taken to clean and prepare the data further:\n",
    "- Missing values in columns like 'pageviews', 'bounces', and 'transactionRevenue' are replaced with zeros.\n",
    "- A new binary column, 'at_least_one_Conversion', is introduced to indicate whether a transaction has occurred.\n",
    "- Redundant and constant value columns are identified and removed to streamline the dataset.\n",
    "\n",
    "### Merging Transformed Data\n",
    "Post-PCA, the reduced dataset is merged back with the main dataset. \n",
    "\n",
    "### Final Dataset Preparation\n",
    "The final steps involve sorting and merging datasets to ensure all transformations are consistently applied across both training and testing sets. T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/Abdul/Desktop/MMA/Enterprise Data Science/Revenue-Radar/Data/train_dejsonified.csv', parse_dates=['date'])\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff date for the split\n",
    "test_start_date = pd.Timestamp('2017-05-01')\n",
    "train_df = df[df['date'] < test_start_date]\n",
    "test_df = df[df['date'] >= test_start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sources = pd.concat([train_df['source'], test_df['source']]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data, pca=None, fit=False):\n",
    "    if fit:\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(data)\n",
    "    if not pca:\n",
    "        raise ValueError(\"PCA model is not provided for transformation.\")\n",
    "    transformed_data = pca.transform(data)\n",
    "    return transformed_data, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, all_sources, pca_model=None, fit_pca=True):\n",
    "    # Handle one-hot encoding for 'source'\n",
    "    source_df = pd.DataFrame(df[['source', 'fullVisitorId']])\n",
    "    one_hot_encoded_df = pd.get_dummies(source_df['source'], prefix='', prefix_sep='', dtype=int)\n",
    "\n",
    "    # Ensure all possible columns are included, filling missing with 0\n",
    "    one_hot_encoded_df = one_hot_encoded_df.reindex(columns=all_sources, fill_value=0)\n",
    "    one_hot_encoded_df['fullVisitorId'] = source_df['fullVisitorId'].values\n",
    "    source_df = one_hot_encoded_df.groupby('fullVisitorId').sum().reset_index()\n",
    "\n",
    "    \n",
    "    # Remove 'fullVisitorId' temporarily for PCA processing\n",
    "    pca_data = source_df.drop('fullVisitorId', axis=1).to_numpy()\n",
    "\n",
    "    # if fit_pca:\n",
    "    #     pca_model = PCA(n_components=3)\n",
    "    #     pca_model.fit(pca_data)\n",
    "    \n",
    "    # transformed_data = pca_model.transform(pca_data)\n",
    "    # Fit and transform PCA on the training data, or just transform on the test data\n",
    "    transformed_data, pca_model = apply_pca(pca_data, pca_model, fit=fit_pca)\n",
    "    \n",
    "    # Convert PCA output to DataFrame and create meaningful column names\n",
    "    pca_column_names = ['Source_PC1', 'Source_PC2', 'Source_PC3']\n",
    "    pca_df = pd.DataFrame(transformed_data, columns=pca_column_names)\n",
    "    pca_df['fullVisitorId'] = source_df['fullVisitorId'].values  # Ensure correct alignment\n",
    "    \n",
    "    # Fill missing values for 'pageviews' and 'bounces'\n",
    "    df['pageviews'].fillna(0, inplace=True)\n",
    "    df['bounces'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Replace missing 'transactionRevenue' with 0 and convert to int\n",
    "    df['transactionRevenue'].fillna(0, inplace=True)\n",
    "    df['transactionRevenue'] = df['transactionRevenue'].astype(int)\n",
    "    \n",
    "    # Create a binary 'Conversion' flag\n",
    "    df['Conversion'] = df['transactionRevenue'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df['at_least_one_conversion'] = df.groupby('fullVisitorId')['Conversion'].transform('sum').apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # Sort data by 'fullVisitorId' and 'date'\n",
    "    df.sort_values(by=['fullVisitorId', 'date'], inplace=True)\n",
    "    \n",
    "    # Identify and drop constant columns except 'bounces'\n",
    "    constant_columns = [col for col in df.columns if df[col].nunique() == 1 and col not in['Bounces','isTrueDirect']]    \n",
    "    df.drop(constant_columns, axis=1, inplace=True)\n",
    "    \n",
    "    # Drop other unnecessary columns\n",
    "    df.drop(['networkDomain', 'visitStartTime', 'visitNumber'], axis=1, inplace=True)\n",
    "    \n",
    "    # Merge data frames for channel visit analysis\n",
    "    first_visit_channel = df.groupby('fullVisitorId')['channelGrouping'].first().reset_index(name='FirstChannelVisit')\n",
    "    last_visit_channel = df.groupby('fullVisitorId')['channelGrouping'].last().reset_index(name='LastChannelVisit')\n",
    "    channels_first_last = pd.merge(first_visit_channel, last_visit_channel, on='fullVisitorId', how='inner')\n",
    "    \n",
    "    df_unique = df.drop_duplicates(subset='fullVisitorId', keep='first')\n",
    "    channels_first_last = pd.merge(channels_first_last, df_unique[['fullVisitorId', 'at_least_one_conversion', 'country', 'continent', 'subContinent']].drop_duplicates(), on='fullVisitorId', how='inner')\n",
    "    \n",
    "    \n",
    "    # Calculating the total number of visits by each user\n",
    "    total_visits_by_user=dict(df['fullVisitorId'].value_counts().reset_index(name='TotalVisits').values)\n",
    "\n",
    "    \n",
    "    # Calculating visits by channel for each user\n",
    "    channel_visits_by_user=dict(df.groupby('fullVisitorId')['channelGrouping'].value_counts())\n",
    "    list_of_dicts = []\n",
    "    for (visitor_id, channel), visits in channel_visits_by_user.items():\n",
    "        list_of_dicts.append({'fullVisitorId': visitor_id, channel: visits})\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df_channel_visits = pd.DataFrame(list_of_dicts)\n",
    "\n",
    "    # Combining all dictionaries representing the same fullVisitorId\n",
    "    df_channel_visits = df_channel_visits.groupby('fullVisitorId', as_index=False).sum()\n",
    "\n",
    "    \n",
    "    # Total pageviews by user\n",
    "    total_pageviews_by_user=dict(df.groupby('fullVisitorId')['pageviews'].sum().reset_index(name='TotalPageviews').values)\n",
    "\n",
    "    # Total bounces by user\n",
    "    total_bounces_by_user=dict(df.groupby('fullVisitorId')['bounces'].sum().reset_index(name='TotalBounces').values)\n",
    "\n",
    "\n",
    "    visits_by_device=dict(df.groupby('fullVisitorId')['deviceCategory'].value_counts())\n",
    "    # converting the dictionary to a list of dictionaries\n",
    "    list_of_dicts = []\n",
    "    for (visitor_id, device), visits in visits_by_device.items():\n",
    "        list_of_dicts.append({'fullVisitorId': visitor_id, device: visits})\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df_visits_by_device = pd.DataFrame(list_of_dicts)\n",
    "\n",
    "    # Combining all dictionaries representing the same fullVisitorId\n",
    "    df_visits_by_device = df_visits_by_device.groupby('fullVisitorId', as_index=False).sum()\n",
    "    \n",
    "    # Session pageviews\n",
    "    first_session_pageviews=dict(df.groupby('fullVisitorId')['pageviews'].first().reset_index(name='FirstSessionPageviews').values)\n",
    "    last_session_pageviews=dict(df.groupby('fullVisitorId')['pageviews'].last().reset_index(name='LastSessionPageviews').values)\n",
    "    \n",
    "    # Campaign data adjustment\n",
    "    df['campaign'] = df['campaign'].apply(lambda x: 0 if x == '(not set)' else 1)\n",
    "    \n",
    "    # Calculating campaign visits\n",
    "    campaign_visits_by_user=dict(df.groupby('fullVisitorId')['campaign'].sum().reset_index(name='CampaignVisits').values)\n",
    "\n",
    "    \n",
    "    # TrueDirect and AdContent data handling\n",
    "    df['isTrueDirect'] = df['isTrueDirect'].fillna(False)\n",
    "    true_Direct_by_user=dict(df.groupby('fullVisitorId')['isTrueDirect'].sum().reset_index(name='isTrueDirect').values)\n",
    "\n",
    "    adcontent_by_user=dict(df.groupby('fullVisitorId')['adContent'].apply(lambda x: x.notnull().sum()).reset_index(name='AdContentVisits').values)\n",
    "\n",
    "\n",
    "    # Convert dictionaries to DataFrames\n",
    "    df_total_bounces = pd.DataFrame(list(total_bounces_by_user.items()), columns=['fullVisitorId', 'TotalBounces'])\n",
    "    df_total_visits = pd.DataFrame(list(total_visits_by_user.items()), columns=['fullVisitorId', 'TotalVisits'])\n",
    "    df_total_pageviews = pd.DataFrame(list(total_pageviews_by_user.items()), columns=['fullVisitorId', 'TotalPageviews'])\n",
    "    df_first_session_pageviews = pd.DataFrame(list(first_session_pageviews.items()), columns=['fullVisitorId', 'FirstSessionPageviews'])\n",
    "    df_last_session_pageviews = pd.DataFrame(list(last_session_pageviews.items()), columns=['fullVisitorId', 'LastSessionPageviews'])\n",
    "    df_campaign_visits = pd.DataFrame(list(campaign_visits_by_user.items()), columns=['fullVisitorId', 'CampaignVisits'])\n",
    "    df_true_Direct = pd.DataFrame(list(true_Direct_by_user.items()), columns=['fullVisitorId', 'isTrueDirect'])\n",
    "    df_adcontent_visits = pd.DataFrame(list(adcontent_by_user.items()), columns=['fullVisitorId', 'AdContentVisits'])\n",
    "\n",
    "    # Merging all computed DataFrames\n",
    "    data_frames_to_merge = [\n",
    "        df_total_visits, df_total_bounces, df_channel_visits,\n",
    "        df_total_pageviews, df_visits_by_device, df_first_session_pageviews,\n",
    "        df_last_session_pageviews, df_campaign_visits, df_true_Direct,\n",
    "        df_adcontent_visits\n",
    "    ]\n",
    "\n",
    "    final_df = channels_first_last\n",
    "    for data_frame in data_frames_to_merge:\n",
    "        final_df = pd.merge(final_df, data_frame, on='fullVisitorId', how='inner')\n",
    "    \n",
    "    # Convert any floating-point columns to integers if necessary\n",
    "    for col in final_df.select_dtypes(include=['float64']).columns:\n",
    "        final_df[col] = final_df[col].astype(int)\n",
    "\n",
    "    return final_df, pca_model, pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply preprocessing to both train and test datasets\n",
    "processed_train, pca_model,pca_df_train = preprocess_data(train_df, all_sources, fit_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test,_, pca_df_test = preprocess_data(test_df, all_sources, pca_model, fit_pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the pca_df_train and pca_df_test with the processed_train and processed_test\n",
    "processed_train = pd.merge(processed_train, pca_df_train, on='fullVisitorId', how='inner')\n",
    "processed_test = pd.merge(processed_test, pca_df_test, on='fullVisitorId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567490, 29)\n",
      "(161282, 29)\n"
     ]
    }
   ],
   "source": [
    "print(processed_train.shape)\n",
    "print(processed_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test.to_csv('/Users/Abdul/Desktop/MMA/Enterprise Data Science/test_df.csv', index=False)\n",
    "processed_train.to_csv('/Users/Abdul/Desktop/MMA/Enterprise Data Science/train_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
